"""
NLP Engine - Crowd Echo Algorithm
Extracts semantic fingerprints from Arabic tweet datasets.
"""

from typing import List, Dict, Any, Set, Tuple
from collections import Counter
from datetime import datetime
import re

# Comprehensive Arabic stop words list
ARABIC_STOP_WORDS: Set[str] = {
    # Prepositions
    "ÙÙŠ", "Ù…Ù†", "Ø¹Ù„Ù‰", "Ø¥Ù„Ù‰", "Ø§Ù„Ù‰", "Ø¹Ù†", "Ù…Ø¹", "Ø¨ÙŠÙ†", "Ø­ØªÙ‰", "Ù…Ù†Ø°",
    "Ø®Ù„Ø§Ù„", "Ø¹Ù†Ø¯", "Ù„Ø¯Ù‰", "Ø¶Ø¯", "Ù†Ø­Ùˆ", "ÙÙˆÙ‚", "ØªØ­Øª", "Ø£Ù…Ø§Ù…", "ÙˆØ±Ø§Ø¡",
    
    # Pronouns
    "Ø£Ù†Ø§", "Ø§Ù†Ø§", "Ø£Ù†Øª", "Ø§Ù†Øª", "Ø£Ù†ØªÙ…", "Ø§Ù†ØªÙ…", "Ù‡Ùˆ", "Ù‡ÙŠ", "Ù‡Ù…", "Ù‡Ù†",
    "Ù†Ø­Ù†", "Ø§Ù†ØªÙŠ", "Ø£Ù†ØªÙŠ", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙ„Ùƒ", "Ù‡Ø¤Ù„Ø§Ø¡", "Ø£ÙˆÙ„Ø¦Ùƒ",
    
    # Conjunctions
    "Ùˆ", "Ø£Ùˆ", "Ø§Ùˆ", "Ø«Ù…", "Ù„ÙƒÙ†", "Ø¨Ù„", "Ù„Ø£Ù†", "Ù„Ø§Ù†", "Ø¥Ø°Ø§", "Ø§Ø°Ø§",
    "Ù„Ùˆ", "ÙƒÙŠ", "Ø­ÙŠÙ†", "Ø¹Ù†Ø¯Ù…Ø§", "Ø¨ÙŠÙ†Ù…Ø§", "ÙƒÙ…Ø§", "Ù…Ø«Ù„", "Ø¥Ù†", "Ø§Ù†", "Ø£Ù†",
    
    # Articles & Particles
    "Ø§Ù„", "Ø§Ù„Ù€", "Ù„Ø§", "Ù„Ù…", "Ù„Ù†", "Ù…Ø§", "Ù‚Ø¯", "Ø³ÙˆÙ", "Ø³Ù€", "ÙƒÙ„",
    "Ø¨Ø¹Ø¶", "ÙƒØ«ÙŠØ±", "Ù‚Ù„ÙŠÙ„", "Ø¬Ø¯Ø§", "Ø¬Ø¯Ø§Ù‹", "ÙÙ‚Ø·", "Ø£ÙŠØ¶Ø§", "Ø§ÙŠØ¶Ø§",
    
    # Common verbs (conjugated)
    "ÙƒØ§Ù†", "ÙƒØ§Ù†Øª", "ÙŠÙƒÙˆÙ†", "ØªÙƒÙˆÙ†", "ÙƒØ§Ù†ÙˆØ§", "ÙŠÙƒÙˆÙ†ÙˆÙ†", "Ù‡Ù†Ø§Ùƒ",
    "ØµØ§Ø±", "Ø£ØµØ¨Ø­", "Ø§ØµØ¨Ø­", "Ø¨Ø§Øª", "Ø¸Ù„", "Ù…Ø§Ø²Ø§Ù„",
    
    # Dialetical (Saudi/Gulf)
    "ÙˆØ´", "Ø§ÙŠØ´", "Ù„ÙŠØ´", "ÙƒÙŠÙ", "Ù…ØªÙ‰", "ÙˆÙŠÙ†", "Ù…Ù†Ùˆ", "Ø´Ù†Ùˆ",
    "Ù…Ùˆ", "Ù…Ø¨", "Ø¨Ø³", "ÙŠØ¹Ù†ÙŠ", "Ø·ÙŠØ¨", "Ø²ÙŠÙ†", "Ø§ÙˆÙƒÙŠ", "Ø®Ù„Ø§Øµ",
    "Ø§Ù„Ù„ÙŠ", "Ø§Ù„Ù„Ù‰", "Ø§Ù„ÙŠ", "Ø§Ù„Ù‡", "Ù„Ù‡", "Ù„Ù‡Ø§", "Ù„Ù‡Ù…", "Ø¹Ù„ÙŠÙ‡", "Ø¹Ù„ÙŠÙ‡Ø§",
    "ÙÙŠÙ‡", "ÙÙŠÙ‡Ø§", "Ù…Ù†Ù‡", "Ù…Ù†Ù‡Ø§", "Ø¨Ø¹Ø¯", "Ù‚Ø¨Ù„",
    
    # Common fillers
    "ÙŠØ§", "ÙŠØ§Ø¡", "Ø¢Ù‡", "Ø§Ù‡", "ÙˆØ§Ù„Ù„Ù‡", "ÙˆØ´", "Ù‡Ø§Ù‡",
    
    # Twitter-specific
    "rt", "via", "cc", "dm",
    
    # Punctuation and emojis (will be handled separately)
}

# Regex pattern for Arabic text cleaning
ARABIC_PATTERN = re.compile(r'[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF]+')
HASHTAG_PATTERN = re.compile(r'#\w+')
MENTION_PATTERN = re.compile(r'@\w+')
URL_PATTERN = re.compile(r'https?://\S+')


def clean_text(text: str) -> str:
    """Remove URLs, mentions, and clean Arabic text."""
    text = URL_PATTERN.sub('', text)
    text = MENTION_PATTERN.sub('', text)
    # Keep hashtags for analysis but remove the # symbol later
    return text.strip()


def tokenize_arabic(text: str) -> List[str]:
    """
    Tokenize Arabic text by splitting on spaces and punctuation.
    Preserves Arabic characters only.
    """
    # Clean the text first
    cleaned = clean_text(text)
    
    # Split by whitespace and common Arabic punctuation
    tokens = re.split(r'[\sØŒ.!ØŸ:Ø›â€¦\-_\(\)\[\]Â«Â»"\']+', cleaned)
    
    # Filter empty tokens and very short ones
    tokens = [t.strip() for t in tokens if t.strip() and len(t.strip()) > 1]
    
    return tokens


def remove_stop_words(tokens: List[str]) -> List[str]:
    """Remove Arabic stop words from token list."""
    return [t for t in tokens if t not in ARABIC_STOP_WORDS]


def extract_hashtags(text: str) -> List[str]:
    """Extract hashtags from text."""
    hashtags = HASHTAG_PATTERN.findall(text)
    return [h[1:] for h in hashtags]  # Remove # symbol


def calculate_ngram_frequency(
    tokens: List[str], 
    n: int = 1
) -> Dict[str, int]:
    """Calculate n-gram frequencies."""
    if n == 1:
        return dict(Counter(tokens))
    
    ngrams = []
    for i in range(len(tokens) - n + 1):
        ngram = " ".join(tokens[i:i+n])
        ngrams.append(ngram)
    
    return dict(Counter(ngrams))


def extract_semantic_fingerprint(
    dataset: List[Dict[str, Any]],
    report_time: datetime = None,
    top_k: int = 3
) -> Dict[str, Any]:
    """
    Extract semantic fingerprint from tweet dataset.
    
    Algorithm (Crowd Echo):
    1. Filter tweets after report time (if provided)
    2. Tokenize Arabic text
    3. Remove stop words
    4. Calculate unigram and bigram frequencies
    5. Return top keywords
    
    Args:
        dataset: List of tweet objects
        report_time: Only analyze tweets after this time
        top_k: Number of top keywords to return
    
    Returns:
        Dictionary containing:
        - top_keywords: List of most frequent meaningful words
        - unigram_freq: Full unigram frequency dict
        - bigram_freq: Full bigram frequency dict
        - total_tweets_analyzed: Count of processed tweets
        - hashtag_freq: Frequency of hashtags
    """
    
    # Step 1: Filter by time if report_time provided
    if report_time:
        filtered_tweets = [
            t for t in dataset 
            if datetime.fromisoformat(t["created_at"].replace("Z", "")) >= report_time
        ]
    else:
        filtered_tweets = dataset
    
    all_tokens = []
    all_hashtags = []
    
    # Step 2 & 3: Tokenize and collect
    for tweet in filtered_tweets:
        text = tweet.get("text", "")
        
        # Extract hashtags separately
        hashtags = extract_hashtags(text)
        all_hashtags.extend(hashtags)
        
        # Tokenize
        tokens = tokenize_arabic(text)
        
        # Remove stop words
        filtered_tokens = remove_stop_words(tokens)
        
        all_tokens.extend(filtered_tokens)
    
    # Step 4: Calculate frequencies
    unigram_freq = calculate_ngram_frequency(all_tokens, n=1)
    bigram_freq = calculate_ngram_frequency(all_tokens, n=2)
    hashtag_freq = dict(Counter(all_hashtags))
    
    # Step 5: Get top keywords (excluding very common generic words)
    # Sort by frequency
    sorted_unigrams = sorted(
        unigram_freq.items(), 
        key=lambda x: x[1], 
        reverse=True
    )
    
    # Filter out generic words that might slip through
    generic_words = {"Ø§Ù„Ù„Ù‡", "Ø§Ù„Ù†Ø§Ø³", "Ø§Ù„ÙŠÙˆÙ…", "Ø§Ù„Ø­ÙŠÙ†", "Ø´ÙŠ", "Ø§ÙƒØ«Ø±", "ÙƒÙ„Ø§Ù…"}
    top_keywords = [
        word for word, count in sorted_unigrams 
        if word not in generic_words
    ][:top_k]
    
    # Also get top bigrams
    sorted_bigrams = sorted(
        bigram_freq.items(),
        key=lambda x: x[1],
        reverse=True
    )[:5]
    
    return {
        "top_keywords": top_keywords,
        "top_bigrams": [bg[0] for bg in sorted_bigrams],
        "unigram_freq": unigram_freq,
        "bigram_freq": bigram_freq,
        "hashtag_freq": hashtag_freq,
        "total_tweets_analyzed": len(filtered_tweets),
        "total_tokens": len(all_tokens)
    }


def find_tweets_with_keywords(
    dataset: List[Dict[str, Any]],
    keywords: List[str],
    start_time: datetime = None,
    end_time: datetime = None
) -> List[Dict[str, Any]]:
    """
    Find tweets containing any of the specified keywords within time range.
    """
    results = []
    
    for tweet in dataset:
        tweet_time = datetime.fromisoformat(tweet["created_at"].replace("Z", ""))
        
        # Time filter
        if start_time and tweet_time < start_time:
            continue
        if end_time and tweet_time > end_time:
            continue
        
        # Keyword filter
        text = tweet.get("text", "")
        if any(kw in text for kw in keywords):
            results.append(tweet)
    
    return results


def count_tweets_in_range(
    dataset: List[Dict[str, Any]],
    keywords: List[str],
    start_time: datetime,
    end_time: datetime
) -> int:
    """Count tweets with keywords in time range."""
    return len(find_tweets_with_keywords(dataset, keywords, start_time, end_time))


if __name__ == "__main__":
    # Test NLP engine
    from data_generator import generate_synthetic_dataset
    
    tweets = generate_synthetic_dataset()
    print(f"Generated {len(tweets)} tweets for NLP testing")
    
    # Extract fingerprint
    fingerprint = extract_semantic_fingerprint(tweets)
    
    print(f"\nðŸ“Š Semantic Fingerprint Analysis")
    print(f"   Tweets analyzed: {fingerprint['total_tweets_analyzed']}")
    print(f"   Total tokens: {fingerprint['total_tokens']}")
    print(f"\n   Top Keywords: {fingerprint['top_keywords']}")
    print(f"   Top Bigrams: {fingerprint['top_bigrams']}")
    print(f"\n   Top Hashtags:")
    for tag, count in sorted(fingerprint['hashtag_freq'].items(), key=lambda x: -x[1])[:5]:
        print(f"      #{tag}: {count}")
